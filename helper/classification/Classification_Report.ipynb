{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c11cc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#Change path specificly to your directories\n",
    "sys.path.insert(1, '/home/fishial/Fishial/Object-Detection-Model')\n",
    "\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import math \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.optim import Optimizer, SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from sklearn.neighbors import KDTree\n",
    "import pandas as pd\n",
    "from module.classification_package.src.utils import read_json, save_json\n",
    "from module.classification_package.interpreter_classifier import ClassifierFC\n",
    "from module.classification_package.interpreter_embeding import EmbeddingClassifier\n",
    "# from module.classification_package.interpreter_embeding_data import EmbeddingClassifierData\n",
    "\n",
    "from module.segmentation_package.interpreter_segm import SegmentationInference\n",
    "from module.classification_package.src.dataset import FishialDataset, FishialDatasetOnlineCuting\n",
    "# from module.segmentation_package.src.utils import resize_image\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import sklearn.metrics.pairwise\n",
    "import scipy.spatial.distance\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "# TRESHOLD 40.6256\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (40, 20),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':'x-large',\n",
    "         'ytick.labelsize':'x-large'}\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1412003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ed5597e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classifier_triplet_data_base = EmbeddingClassifier(\n",
    "    '../../output/train_results/resnet_18_98_finall_update/full_256.ckpt',\n",
    "    '../../output/train_results/resnet_18_98_finall_update/train+test_embedding.pt',\n",
    "    '../../output/train_results/resnet_18_98_finall_update/train+test_idx.json',\n",
    "    device='cpu', THRESHOLD = 1e5, \n",
    "    resnet = models.resnet18())\n",
    "\n",
    "data_labels = read_json('../../output/train_results/resnet_18_98_finall_update/train+test_labels.json')\n",
    "data_labels.update({'98': 'Other'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0222947",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_images_folder = r'/home/fishial/Fishial/dataset/fishial_collection/data'\n",
    "path_to_COCO_file = r'/home/fishial/Fishial/dataset/export/fixed_all_json.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7455aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "softmax = nn.Softmax(dim=None)\n",
    "\n",
    "main_folder = \"/home/codahead/Fishial/FishialReaserch/datasets/fishial_75_V2.0\"\n",
    "data_sets = {\n",
    "    'train'       : {'path' : 'data_train.json'       },\n",
    "    'test'        : {'path' : 'data_test.json'        },\n",
    "    'remain'      : {'path' : 'data_remain.json'      },\n",
    "    'out_of_class': {'path' : 'data_out_of_class.json'}\n",
    "}\n",
    "\n",
    "for i in data_sets:\n",
    "    ds = FishialDatasetOnlineCuting(\n",
    "        path_to_images_folder = path_to_images_folder, \n",
    "        path_to_COCO_file = path_to_COCO_file,\n",
    "        dataset_type = i,\n",
    "        transform=None)\n",
    "    \n",
    "    data_sets[i].update({\n",
    "        'labels': {\n",
    "            data_labels[label]: {       \n",
    "            'pred':     [],\n",
    "            'distance': [] \n",
    "    } for label in data_labels}})\n",
    "    \n",
    "    labels_dict = ds.labels_dict\n",
    "    for idx in range(len(ds)):\n",
    "        label_correct = labels_dict[int(ds[idx][1])]\n",
    "\n",
    "        img = np.asarray(ds[idx][0])\n",
    "        output_triplet_data = model_classifier_triplet_data_base.inference_numpy(img, top_k = 15)\n",
    "        \n",
    "        if label_correct in data_sets[i]['labels']:\n",
    "            data_sets[i]['labels'][label_correct]['distance'].append(output_triplet_data[0][2])\n",
    "            data_sets[i]['labels'][label_correct]['pred'].append(data_labels[str(output_triplet_data[0][0])])\n",
    "        else:\n",
    "            data_sets[i]['labels']['Other']['distance'].append(output_triplet_data[0][2])\n",
    "            data_sets[i]['labels']['Other']['pred'].append(data_labels[str(output_triplet_data[0][0])])\n",
    "\n",
    "        print(\"name: {} Left: [{}/{}] \".format(data_sets[i]['path'], \n",
    "                                                     idx, len(ds) ), end='\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085cf24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_f1_score(map_truth):\n",
    "    try:\n",
    "        f1 = (map_truth[0] / (map_truth[0] + 0.5 * ( map_truth[2] + map_truth[3] )))\n",
    "        return f1\n",
    "    except ZeroDivisionError:\n",
    "        return None\n",
    "\n",
    "#[t_p, t_n, f_p, f_n]\n",
    "def get_g_mean(map_truth):\n",
    "    try:\n",
    "        t_p, t_n, f_p, f_n = map_truth\n",
    "        sens = (t_p / (t_p + f_n))\n",
    "        spec = t_n / (t_n + f_p)\n",
    "        return math.sqrt(sens * spec)\n",
    "    except ZeroDivisionError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21af5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous = 0\n",
    "\n",
    "index_name = []\n",
    "\n",
    "mean_value = np.median([k for i in data_sets for z in data_sets[i]['labels'] for k in data_sets[i]['labels'][z]['distance']])\n",
    "print(f'Median distance over all datasets: {mean_value}')\n",
    "\n",
    "thresholds = list(np.linspace(mean_value, mean_value + mean_value * 0.65, 1000))\n",
    "\n",
    "for data_set in data_sets:\n",
    "    for label_idx, label in enumerate(data_sets[data_set]['labels']):\n",
    "        left_cnt = len(data_sets[data_set]['labels']) - label_idx\n",
    "        print(f'Left: {left_cnt} dataset: {data_set}', end='\\r')\n",
    "        dict_result = {\n",
    "            'True Positive' : [],\n",
    "            'True Negative' : [],\n",
    "            'False Positive': [],\n",
    "            'False Negative': [],\n",
    "            'Threshold': []}\n",
    "        \n",
    "        specie = data_sets[data_set]['labels'][label]\n",
    "        \n",
    "        f1_scores = []\n",
    "        for idx, i in enumerate(thresholds):\n",
    "            f_p, f_n, t_p, t_n = 0, 0, 0, 0\n",
    "            for id_dist, dist in enumerate(specie['distance']):\n",
    "                if dist <= i:\n",
    "                    if specie['pred'][id_dist] == label:\n",
    "                        t_p += 1\n",
    "                    else:\n",
    "                        f_p += 1\n",
    "                else:\n",
    "                    if specie['pred'][id_dist] == label:\n",
    "                        f_n += 1\n",
    "                    else:\n",
    "                        t_n += 1\n",
    "            try:\n",
    "                #(t_p / (t_p + 0.5 * ( f_p + f_n )))\n",
    "                f1_scores.append([t_p, t_n, f_p, f_n])\n",
    "            except ZeroDivisionError:\n",
    "                pass\n",
    "#                 f1_scores.append(0.0)\n",
    "        \n",
    "        data_sets[data_set]['labels'][label].update({'f1_scores': f1_scores})\n",
    "    \n",
    "max_score, top_thresh, g_mean = 0.0, 0.0, 0.0\n",
    "\n",
    "for thresholds_idx, threshold in enumerate(thresholds):\n",
    "    list_of_f1_scores = []\n",
    "    \n",
    "    for data_set in ['test', 'remain', 'out_of_class']:\n",
    "        for label in data_sets[data_set]['labels']:\n",
    "            list_of_f1_scores.append(data_sets[data_set]['labels'][label]['f1_scores'][thresholds_idx])\n",
    "     \n",
    "    f1_scores = []\n",
    "    g_mean = []\n",
    "    for map_truth in list_of_f1_scores:\n",
    "        if get_f1_score(map_truth):\n",
    "            f1_scores.append(get_f1_score(map_truth))\n",
    "\n",
    "        if get_g_mean(map_truth):\n",
    "            g_mean.append(get_g_mean(map_truth))\n",
    "\n",
    "    mean_f1_score = np.mean(f1_scores)\n",
    "    mean_g_mean = np.mean(g_mean)\n",
    "\n",
    "    if  mean_g_mean > max_score:\n",
    "        max_score, top_thresh, g_mean = mean_g_mean, threshold, mean_f1_score\n",
    "            \n",
    "print(f'| Data: {data_set} | THRESHOLD: {top_thresh} | F1 score: {max_score} | mean_g_mean: {mean_g_mean}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f829a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_hist(data, title):\n",
    "    std = np.std(data)\n",
    "    mean = np.mean(data)\n",
    "    median = np.median(data)\n",
    "    var = np.var(data)\n",
    "    \n",
    "    bin_size = 40\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    counts, bins, _ = ax.hist(data, bins=bin_size) # density=1\n",
    "    avg = bins[np.argmax(counts)]\n",
    "    \n",
    "    ax.axvline(avg - std, 0, 1, label='std left', color='r')\n",
    "    ax.axvline(median, 0, 1, label='median', color='g')\n",
    "    ax.axvline(top_thresh, 0, 1, label='top_thresh', color='c', linestyle = '--')\n",
    "    ax.axvline(avg + std, 0, 1, label='std right', color='r') \n",
    "    ax.legend()\n",
    "    \n",
    "    y = ((1 / (np.sqrt(2 * np.pi) * std)) * np.exp(-0.5 * (1 / std * (bins - mean))**2))\n",
    "#     ax.plot(bins, y, '--')\n",
    "    ax.set_title(f'Distribution of distances for the data set: {title} $\\sigma={round(std, 3)}$:  | $\\mu={round(median, 3)}$:  | avg: {round(avg, 3)}')\n",
    "    ax.set_xlabel(f'Distance (bin size = {bin_size})')\n",
    "    ax.set_ylabel('Count')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    sigma_1x = sum([True if i > (avg - std) and i < (avg + std) else False for i in data])\n",
    "    sigma_2x = sum([True if i > (avg - 2 * std) and i < (avg + 2 * std) else False for i in data])\n",
    "    sigma_3x = sum([True if i > (avg - 3 * std) and i < (avg + 3 * std) else False for i in data])\n",
    "    coef_var = std/mean\n",
    "    print(f'coefficient variance: {coef_var}')\n",
    "    print(f'68–95–99.7 rule: |{sigma_1x/ len(data)}|{sigma_2x/ len(data)}|{sigma_3x/ len(data)}|')\n",
    "    return counts, bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266e0b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for data_set in data_sets:\n",
    "    print(f\"Name: {data_set}\")\n",
    "    data_plot = np.array([float(k.detach().numpy()) for z in data_sets[data_set]['labels'] for k in data_sets[data_set]['labels'][z]['distance']])\n",
    "    truth_map = [z == k for z in data_sets[data_set]['labels'] for k in data_sets[data_set]['labels'][z]['pred']]\n",
    "\n",
    "    counts, bins = draw_hist(data_plot, data_set.capitalize())\n",
    "    bins_count_true_pos   = [0] * len(bins)\n",
    "    bins_count_true_neg   = [0] * len(bins)\n",
    "    bins_count_false_pos  = [0] * len(bins)\n",
    "    bins_count_false_neg  = [0] * len(bins)\n",
    "    \n",
    "    for bin_id, bin_i in enumerate(bins):\n",
    "        for label_truth_id, label_truth in enumerate(truth_map):\n",
    "\n",
    "            if data_plot[label_truth_id] <= bin_i:       \n",
    "                if label_truth:\n",
    "                    bins_count_true_pos[bin_id]   += 1\n",
    "                else:\n",
    "                    bins_count_false_pos[bin_id]  += 1\n",
    "            else:\n",
    "                if label_truth:\n",
    "                    bins_count_false_neg[bin_id]  += 1\n",
    "                else:\n",
    "                    bins_count_true_neg[bin_id]   += 1\n",
    "    \n",
    "    plotdata = pd.DataFrame({\n",
    "        \"True Positive\" : bins_count_true_pos,\n",
    "        \"True Negative\" : bins_count_true_neg,\n",
    "        \"False Positive\": bins_count_false_pos,\n",
    "        \"False negative\": bins_count_false_neg},\n",
    "        index=[round(bin, 2) for bin in bins])\n",
    "    \n",
    "    ax = plotdata.plot(kind='bar', stacked=True, color= ['g', 'b', 'c', 'r'])\n",
    "    for sss, p in enumerate(ax.patches):\n",
    "        bias = sss % 4\n",
    "        ax.annotate(str(int(p.get_height())), (p.get_x() * 1.005, p.get_y() + p.get_height()/2 ))\n",
    "    \n",
    "    plt.title(f\"TP/TN/FP/FN for data set {data_set.capitalize()}\")\n",
    "    plt.xlabel(\"Distance\")\n",
    "    plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404e859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb808dc",
   "metadata": {},
   "source": [
    "## correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d4694c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "\n",
    "for asd in ['test', 'remain']:\n",
    "    for label in data_sets[asd]['labels']:\n",
    "        if label == 'Other': continue\n",
    "\n",
    "        y_true.extend([label] * len(data_sets[asd]['labels'][label]['pred']))\n",
    "        y_pred.extend(data_sets[asd]['labels'][label]['pred'])\n",
    "\n",
    "#     print(y_true)\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    fig, ax = plt.subplots(figsize=(30, 30))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp = disp.plot(cmap=plt.cm.Blues, ax=ax, xticks_rotation=90)\n",
    "    plt.title(f\"Data: {asd}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e88932a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe21f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_numbers = random.choices([100,100], k=200) \n",
    "random_numbers = torch.Tensor(list_numbers)\n",
    "\n",
    "main_folder = '../../output/train_results/resnet_18_200'\n",
    "data_distance_disturb = {}\n",
    "set_names = ['data_train.json_embedding.pt', 'data_test.json_embedding.pt', 'data_remain.json_embedding.pt']\n",
    "for i in zip(\n",
    "    set_names,\n",
    "    ['data_train.json_labels.json', 'data_test.json_labels.json', 'data_remain.json_labels.json']):\n",
    "    \n",
    "    data_base = torch.load(os.path.join(main_folder, i[0])).to('cpu')\n",
    "    data_labels_by_data = read_json(os.path.join(main_folder, i[1]))\n",
    "    \n",
    "    data_distance_disturb.update({\n",
    "        i[0]: {\n",
    "            'labels': {\n",
    "            data_labels[label]: {       \n",
    "            'distance': [] \n",
    "    } for label in data_labels}}})\n",
    "    \n",
    "    for label_id in range(len(data_base)):\n",
    "        print(f\"Current: {label_id} | {i[0]}\", end = '\\r')\n",
    "        for first_id, first_vec in enumerate(data_base[label_id]):\n",
    "            for second_id, second_vec in enumerate(data_base[label_id]):\n",
    "                if first_vec.sum() > 1000 or second_vec.sum() > 1000: continue\n",
    "                diff = (first_vec - second_vec).pow(2).sum().sqrt()\n",
    "                if diff == 0: continue\n",
    "                data_distance_disturb[i[0]]['labels'][data_labels_by_data[str(label_id)]]['distance'].append(diff)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb1e479",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_set = 'data_test.json_embedding.pt'\n",
    "for z in data_distance_disturb[data_set]['labels']:\n",
    "\n",
    "    data_train = np.array([float(k.detach().numpy()) for k in data_distance_disturb[set_names[0]]['labels'][z]['distance']])\n",
    "    data_test = np.array([float(k.detach().numpy()) for k in data_distance_disturb[set_names[1]]['labels'][z]['distance']])\n",
    "    data_remain = np.array([float(k.detach().numpy()) for k in data_distance_disturb[set_names[2]]['labels'][z]['distance']])\n",
    "\n",
    "    bin_size = 60\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    counts, bins, _ = ax.hist(data_train, bins=bin_size, label='data_train', density=1, alpha=0.5) # density=1\n",
    "    counts, bins, _ = ax.hist(data_test, bins=bin_size, label='data_test', density=1, alpha=0.5) # density=1\n",
    "    counts, bins, _ = ax.hist(data_remain, bins=bin_size, label='data_remain', density=1, alpha=0.5) # density=1\n",
    "\n",
    "    ax.axvline(4, 0, 1, label='Threshold', color='g', linestyle = '--')\n",
    "    ax.legend()\n",
    "    \n",
    "    ax.set_title(f'Distribution of distances for the data set: {z}')\n",
    "    ax.set_xlabel(f'Distance (bin size = {bin_size})')\n",
    "    ax.set_ylabel('Count')\n",
    "    fig.tight_layout()\n",
    "    print(f'Train: {len(data_train)} Test: {len(data_test)} Remain: {len(data_remain)}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b5e1bb",
   "metadata": {},
   "source": [
    "## Distribution of distances for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0884973",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_stats(distances):\n",
    "    data = np.array([i.detach().numpy() for i in distances])\n",
    "    precision = 2\n",
    "    std    = round(float(np.std(data)), 2)\n",
    "    mean   = round(float(np.mean(data)), 2)\n",
    "    median = round(float(np.median(data)), 2)\n",
    "    return std, mean, median\n",
    "\n",
    "dict_stats = {\n",
    "    'cnt_train'    : [],\n",
    "    'std_train'    : [],\n",
    "    'mean_train'   : [],\n",
    "    'median_train' : [],\n",
    "    'cnt_test'     : [],\n",
    "    'std_test'     : [],\n",
    "    'mean_test'    : [],\n",
    "    'median_test'  : [],\n",
    "    'cnt_remain'   : [],\n",
    "    'std_remain'   : [],\n",
    "    'mean_remain'  : [],\n",
    "    'median_remain': []\n",
    "}\n",
    "for label in [data_labels[iddx] for iddx in data_labels]:\n",
    "    \n",
    "    std_train, mean_train, median_train = get_stats(data_distance_disturb['data_train.json_embedding.pt']['labels'][label]['distance'])\n",
    "    dict_stats['cnt_train'].append(len(data_distance_disturb['data_train.json_embedding.pt']['labels'][label]['distance']))\n",
    "    dict_stats['mean_train'].append(mean_train)\n",
    "    dict_stats['median_train'].append(median_train)\n",
    "    dict_stats['std_train'].append(std_train)\n",
    "    \n",
    "    std_test, mean_test, median_test = get_stats(data_distance_disturb['data_test.json_embedding.pt']['labels'][label]['distance'])\n",
    "    dict_stats['cnt_test'].append(len(data_distance_disturb['data_test.json_embedding.pt']['labels'][label]['distance']))\n",
    "    dict_stats['mean_test'].append(mean_test)\n",
    "    dict_stats['median_test'].append(median_test)\n",
    "    dict_stats['std_test'].append(std_test)\n",
    "    \n",
    "    std_remain, mean_remain, median_remain = get_stats(data_distance_disturb['data_remain.json_embedding.pt']['labels'][label]['distance'])\n",
    "    dict_stats['cnt_remain'].append(len(data_distance_disturb['data_remain.json_embedding.pt']['labels'][label]['distance']))\n",
    "    dict_stats['mean_remain'].append(mean_remain)\n",
    "    dict_stats['median_remain'].append(median_remain)\n",
    "    dict_stats['std_remain'].append(std_remain)\n",
    "#     count, bins = draw_hist(data_sets['train']['labels'][label]['distance'], label)\n",
    "df = pd.DataFrame(dict_stats, index =[data_labels[iddx] for iddx in data_labels] )\n",
    "\n",
    "writer = pd.ExcelWriter('output.xlsx')\n",
    "# write dataframe to excel sheet named 'marks'\n",
    "df.to_excel(writer, 'marks')\n",
    "# save the excel file\n",
    "writer.save()\n",
    "print('DataFrame is written successfully to Excel Sheet.')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "detectron2_env",
   "language": "python",
   "name": "detectron2_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
