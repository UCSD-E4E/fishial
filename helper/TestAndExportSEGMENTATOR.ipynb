{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97db97da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bed40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import fiftyone as fo\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import lightning as L\n",
    "\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from lightning.pytorch.callbacks import LearningRateFinder\n",
    "\n",
    "from lightning.pytorch.utilities.model_summary import ModelSummary\n",
    "from lightning.pytorch.tuner import Tuner\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model.model_light import FishSeg\n",
    "from model.dataset import SimpleFishialFishDataset\n",
    "from model.utils import *\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36676f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = '.ckpt'\n",
    "\n",
    "encoder_type = \"FPN\"\n",
    "backbone = \"resnet18\"\n",
    "in_channels = 3\n",
    "out_classes = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b461c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fo.load_dataset('classification-v0.8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ddc332",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FishSeg(encoder_type, backbone, in_channels=3, out_classes=1, load_checkpoint = MODEL_PATH)\n",
    "model.eval()\n",
    "model.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e1e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 416\n",
    "\n",
    "loader = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2060cd94",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_to_visualize = 10\n",
    "view = data.take(count_to_visualize)\n",
    "\n",
    "for sample_id, sample in enumerate(view):\n",
    "    \n",
    "    filepath = sample.filepath\n",
    "    polygon = sample.polyline.points[0]\n",
    "    \n",
    "    pil_image = Image.open(filepath)\n",
    "    width, height = pil_image.size\n",
    "    \n",
    "    gt_mask = create_mask(polygon, height, width, color = (100,123,234))\n",
    "    \n",
    "    x_tensor = loader(pil_image).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_tensor)\n",
    "    pr_mask = logits.sigmoid()[0][0].numpy()\n",
    "    pr_mask = resize_logits_mask_pil(pr_mask, width, height)\n",
    "        \n",
    "    visualize(\n",
    "        image=pil_image, \n",
    "        ground_truth_mask=gt_mask, \n",
    "        predicted=pr_mask\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bed862",
   "metadata": {},
   "outputs": [],
   "source": [
    "croped_model = model.model\n",
    "croped_model.eval()\n",
    "croped_model.cpu()\n",
    "\n",
    "example_forward_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "# Trace a specific method and construct `ScriptModule` with\n",
    "# a single `forward` method\n",
    "module = torch.jit.trace(croped_model.forward, example_forward_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfe14b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.onnx\n",
    "\n",
    "# Define the path to save the ONNX model\n",
    "onnx_model_path = f\"saved_models/segmentator/model_resnet18_{IMAGE_SIZE}.onnx\"\n",
    "\n",
    "# Load the pre-trained model (in this case, ResNet18)\n",
    "# model = models.resnet18(pretrained=True)\n",
    "# model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Create an input tensor with the corresponding shape (batch_size, channels, height, width)\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "torch.onnx.export(croped_model,          # The model to export\n",
    "                  dummy_input,           # The input tensor\n",
    "                  onnx_model_path,       # The path to save the model\n",
    "                  export_params=True,    # Export the parameters as well\n",
    "                  opset_version=11,      # The ONNX version\n",
    "                  do_constant_folding=False, # Enable constant folding optimization\n",
    "                  input_names = ['input'],   # The names of the input layers\n",
    "                  output_names = ['output'], # The names of the output layers\n",
    "                  dynamic_axes={'input' : {0 : 'batch_size'},    # Dynamic batch size\n",
    "                                'output' : {0 : 'batch_size'}})\n",
    "\n",
    "print(f\"Model successfully exported to {onnx_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8009543",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_PATH = f\"saved_models/segmentator/model_resnet18_{IMAGE_SIZE}.ts\"\n",
    "module.save(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0653df55",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = torch.jit.load(SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cabe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnxruntime\n",
    "ort_session = onnxruntime.InferenceSession(onnx_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afdf353",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "x = torch.randn(batch_size, 3, IMAGE_SIZE, IMAGE_SIZE, requires_grad=False)\n",
    "\n",
    "# compute ONNX Runtime output prediction\n",
    "ort_inputs = {ort_session.get_inputs()[0].name: x.numpy()}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# with torch.no_grad():\n",
    "torch_out = croped_model(x)\n",
    "\n",
    "np_onnx_oputput = np.array(ort_outs)\n",
    "full_model_output = torch_out.detach().numpy()\n",
    "\n",
    "print(np.sum(np_onnx_oputput - full_model_output))\n",
    "# # compare ONNX Runtime and PyTorch results\n",
    "# np.testing.assert_allclose(full_model_output, np_onnx_oputput, rtol=1e-03, atol=1e-05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a7556f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()\n",
    "new_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fd0ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = transforms.Compose([\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE), Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef7a4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815a8e43",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "count_to_visualize = 10\n",
    "view = data.take(count_to_visualize)\n",
    "\n",
    "for sample_id, sample in enumerate(view):\n",
    "    \n",
    "    filepath = sample.filepath\n",
    "    polygon = sample.polyline.points[0]\n",
    "    \n",
    "    pil_image = Image.open(filepath)\n",
    "    width, height = pil_image.size\n",
    "    \n",
    "    gt_mask = create_mask(polygon, height, width, color = (100,123,234))\n",
    "    \n",
    "    x_tensor = loader(pil_image).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = new_model(x_tensor)\n",
    "        logits_std = model(x_tensor)\n",
    "        logits_croped = croped_model(x_tensor)\n",
    "\n",
    "        ort_inputs = {ort_session.get_inputs()[0].name: x_tensor.numpy()}\n",
    "        ort_outs = torch.tensor(ort_session.run(None, ort_inputs)[0])\n",
    "        \n",
    "    pr_mask_onnx = ort_outs.sigmoid()[0][0].numpy()\n",
    "    pr_mask_onnx = resize_logits_mask_pil(pr_mask_onnx, width, height)\n",
    "    \n",
    "    pr_mask_croped = logits_croped.sigmoid()[0][0].numpy()\n",
    "    pr_mask_croped = resize_logits_mask_pil(pr_mask_croped, width, height)\n",
    "    \n",
    "    pr_mask_std = logits_std.sigmoid()[0][0].numpy()\n",
    "    pr_mask_std = resize_logits_mask_pil(pr_mask_std, width, height)\n",
    "    \n",
    "    pr_mask = logits.sigmoid()[0][0].numpy()\n",
    "    pr_mask = resize_logits_mask_pil(pr_mask, width, height)\n",
    "    \n",
    "    visualize(\n",
    "        image=pil_image, \n",
    "        ground_truth_mask=gt_mask,\n",
    "        predicted_full=pr_mask_std,\n",
    "        pr_mask_croped = pr_mask_croped,\n",
    "        predicted_ts=pr_mask,\n",
    "        pr_mask_onnx=pr_mask_onnx\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca56bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(ort_outs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86415eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "#         %timeit logits = new_model(x_tensor)\n",
    "        %timeit logits_std = model(x_tensor)\n",
    "#         logits_croped = croped_model(x_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NEMO",
   "language": "python",
   "name": "nemo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
